{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\conda_envs\\transformers_lm\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =\"../all_data/dbpedia_csv/train.csv\"\n",
    "\n",
    "df = pd.read_csv(f\"{data_dir}\", header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E. D. Abbott Ltd</td>\n",
       "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                  1  \\\n",
       "0  1                   E. D. Abbott Ltd   \n",
       "1  1                     Schwan-Stabilo   \n",
       "2  1                         Q-workshop   \n",
       "3  1  Marvell Software Solutions Israel   \n",
       "4  1        Bergan Mercy Medical Center   \n",
       "\n",
       "                                                   2  \n",
       "0   Abbott of Farnham E D Abbott Limited was a Br...  \n",
       "1   Schwan-STABILO is a German maker of pens for ...  \n",
       "2   Q-workshop is a Polish company located in Poz...  \n",
       "3   Marvell Software Solutions Israel known as RA...  \n",
       "4   Bergan Mercy Medical Center is a hospital loc...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     40000\n",
       "2     40000\n",
       "3     40000\n",
       "4     40000\n",
       "5     40000\n",
       "6     40000\n",
       "7     40000\n",
       "8     40000\n",
       "9     40000\n",
       "10    40000\n",
       "11    40000\n",
       "12    40000\n",
       "13    40000\n",
       "14    40000\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560000, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the preprocessed tar file\n",
    "\n",
    "# load data\n",
    "preprocessed_data = torch.load(\"../all_data/dbpedia_csv/processed/TRAIN_data.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_data['sents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data['labels'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, Dict\n",
    "from collections import Counter\n",
    "from nltk.tokenize import PunktSentenceTokenizer, TreebankWordTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def get_clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess text for being used in the model, including lower-casing,\n",
    "    standardizing newlines and removing junk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A string to be cleaned\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clean_text : str\n",
    "        String after being cleaned\n",
    "    \"\"\"\n",
    "    if isinstance(text, float):\n",
    "        return ''\n",
    "\n",
    "    clean_text = text.lower().replace('<br />', '\\n').replace('<br>', '\\n').replace('\\\\n', '\\n').replace('&#xd;', '\\n')\n",
    "    return clean_text\n",
    "\n",
    "# tokenizers\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def read_csv(csv_folder: str, split: str, word_limit: int) -> Tuple[list, list, Counter]:\n",
    "    \"\"\"\n",
    "    Read CSVs containing raw training data, clean sentences and labels, and do\n",
    "    a word-count.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_folder : str\n",
    "        Folder containing the dataset in CSV format files\n",
    "\n",
    "    split : str\n",
    "        'train' or 'test' split?\n",
    "\n",
    "    word_limit : int\n",
    "        Truncate long sentences to these many words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sents : list\n",
    "        Sentences ([ word1, ..., wordn ])\n",
    "\n",
    "    labels : list\n",
    "        List of label of each sentence\n",
    "\n",
    "    word_counter : Counter\n",
    "    \"\"\"\n",
    "    assert split in {'train', 'test'}\n",
    "\n",
    "    sents = []\n",
    "    labels = []\n",
    "    word_counter = Counter()\n",
    "    data = pd.read_csv(os.path.join(csv_folder, split + '.csv'), header = None)\n",
    "    for i in tqdm(range(data.shape[0])):\n",
    "        row = list(data.loc[i, :])\n",
    "\n",
    "        s = ''\n",
    "\n",
    "        for text in row[1:]:\n",
    "            text = get_clean_text(text)\n",
    "            s = s + text\n",
    "\n",
    "        words = word_tokenizer.tokenize(s)[:word_limit]\n",
    "        # if sentence is empty (due to removing punctuation, digits, etc.)\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "        word_counter.update(words)\n",
    "\n",
    "        labels.append(int(row[0]) - 1) # since labels are 1-indexed in the CSV\n",
    "        sents.append(words)\n",
    "\n",
    "    return sents, labels, word_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('39nlp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d44b152dbbde612076d8602bde2edf912cdc31b3ee3f21712bf40e7ec5a8c74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
